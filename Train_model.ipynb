{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2469d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import itertools\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbb49ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"C:\\\\Users\\\\sanji\\\\Desktop\\\\SIGN LANGUAGE RECOGNITION\\\\DATA_FOR_TRANS\\\\TRAIN\"\n",
    "test_path = \"C:\\\\Users\\\\sanji\\\\Desktop\\\\SIGN LANGUAGE RECOGNITION\\\\DATA_FOR_TRANS\\\\TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8d81bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9700 images belonging to 5 classes.\n",
      "Found 150 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_batches = ImageDataGenerator(preprocessing_function = tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory = train_path,target_size = (64,64),class_mode = 'categorical',classes=['1','2','3','4','5'],batch_size = 10)\n",
    "test_batches = ImageDataGenerator(preprocessing_function = tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory = test_path,target_size = (64,64),class_mode = 'categorical',classes=['1','2','3','4','5'],batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76a17af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters = 32,kernel_size =(3,3),activation = 'relu',input_shape = (64,64,3),padding = 'same'),\n",
    "    tf.keras.layers.MaxPool2D(strides = 2,pool_size=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters = 64,kernel_size = (3,3),activation = 'relu',padding = 'same'),\n",
    "    tf.keras.layers.MaxPool2D(strides = 2,pool_size=(2,2)),\n",
    "    tf.keras.layers.Conv2D(filters=128,kernel_size=(3,3),activation='relu',padding = 'same'),\n",
    "    tf.keras.layers.MaxPool2D(strides=2,pool_size=(2,2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64,activation='relu'),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128,activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(5,activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5afb55e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 32, 32, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                524352    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 643,077\n",
      "Trainable params: 643,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8b39e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=SGD(learning_rate=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "learn_rate_reduct = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=1,min_lr=0.0005)\n",
    "stop_train = EarlyStopping(monitor = 'val_loss',patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34b971cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "970/970 [==============================] - 86s 88ms/step - loss: 1.5397 - accuracy: 0.3694 - val_loss: 1.2670 - val_accuracy: 0.3867 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "970/970 [==============================] - 99s 102ms/step - loss: 1.0578 - accuracy: 0.5689 - val_loss: 0.7018 - val_accuracy: 0.8000 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "970/970 [==============================] - 109s 112ms/step - loss: 0.8475 - accuracy: 0.6541 - val_loss: 0.1680 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "970/970 [==============================] - 98s 101ms/step - loss: 0.5681 - accuracy: 0.7778 - val_loss: 0.0042 - val_accuracy: 1.0000 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "970/970 [==============================] - 98s 101ms/step - loss: 0.3228 - accuracy: 0.8826 - val_loss: 2.8542e-04 - val_accuracy: 1.0000 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e19b9d33d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(x=train_batches,epochs=5,steps_per_epoch=train_batches.n/10,callbacks=[learn_rate_reduct,stop_train],validation_data = test_batches,validation_steps=test_batches.n/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21466b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp(imgs):\n",
    "    fig,axes = plt.subplots(1,10,figsize=(30,20))\n",
    "    axes=axes.flatten()\n",
    "    for img,ax in zip(imgs,axes):\n",
    "        img = cv.cvtColor(img,cv.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bbd248c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 201ms/step - loss: 3.0013e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "imgs,labels = next(test_batches)\n",
    "loss,acc=model1.evaluate(imgs,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d02e010",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save(\"Trained_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de2eba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model(r\"Trained_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c10b70c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 32, 32, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 16, 16, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                524352    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 643,077\n",
      "Trainable params: 643,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc1738ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n"
     ]
    }
   ],
   "source": [
    "name=['one','two','three','four','five']\n",
    "predictions = model.predict(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28771ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one five three four four four three three five four "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3000x2000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one five three four four four three three five four "
     ]
    }
   ],
   "source": [
    "for k,i in enumerate(predictions):\n",
    "    print(name[np.argmax(i)],end=' ')\n",
    "    \n",
    "disp(imgs)\n",
    "for i in labels:\n",
    "    print(name[np.argmax(i)],end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb30a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
